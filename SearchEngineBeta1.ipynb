{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Search Engine Beta 1.0\n",
    "<p><b>Abstract:</b> Search engine build using Reuters data set and PySpark</p>\n",
    "<p><b>Authors:</b> Uriel Antonio & Ernesto Louie Cortez</p>\n",
    "<p><b>Date:</b>    05/24/2016</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f716ccea390>\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree\n",
    "import re\n",
    "from StringIO import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from bokeh.plotting import output_notebook, show\n",
    "from bokeh.charts import Scatter\n",
    "import findspark\n",
    "import os\n",
    "findspark.init('/home/ubuntu/workspace/spark-1.6.0-bin-hadoop2.6')\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.3.0 pyspark-shell'\n",
    "\n",
    "import pyspark\n",
    "try: \n",
    "    print(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext()\n",
    "    print(sc)\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "<p><b>Abstract: </b>Reuters data is organized in a generalized markup format.  Using BeatifulSoup, articles are scraped for their Title, content, date and place acording to their respective markup tags</p>\n",
    "\n",
    "<p><b>Challenges: </b>1) We initially made the assumption that all articles would have a date, place, title, and body.  During alpha 1 stage, it was found that titles and content became mismatched approximately 30 articles in.  Conditional statements were created to insert placeholders when content was missing.  2) The cleaned data was also littered with mark up tags for special symbols that also needed to be cleaned.    </p>\n",
    "\n",
    "\n",
    "<p><b></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totstring=\"\"\n",
    "\n",
    "with open(\"Data/reut2-000.sgm\",'r') as inF:\n",
    "    for line in inF:\n",
    "        string2=re.sub(\"&.*?>\",\"\",line,flags=re.UNICODE)\n",
    "        string3=re.sub(\"\\n\",\" \",string2,flags=re.UNICODE)\n",
    "        string=re.sub(\"[^0-9a-zA-Z<>/\\s=!-\\\"\\\"]+\",\"\",string3.lower())\n",
    "        totstring+=string\n",
    "    \n",
    "soup= BeautifulSoup(totstring)\n",
    "\n",
    "items_date=list()\n",
    "items_places=list()\n",
    "items_title=list()\n",
    "items_body=list()\n",
    "\n",
    "\n",
    "for a in soup.findAll(\"reuters\"):\n",
    "    if a.date != None:\n",
    "        items_date.append(a.date.getText())\n",
    "    else:\n",
    "        items_date.append(\"N/D\")\n",
    "    if a.places != None:\n",
    "        items_places.append(a.places.getText()) \n",
    "    else:\n",
    "        items_places.append(\"N/L\")\n",
    "    if a.title != None:\n",
    "        items_title.append(a.title.getText())  \n",
    "    else:\n",
    "        items_title.append(\"Untitled\")\n",
    "    if a.content != None:\n",
    "        items_body.append(a.content.getText())\n",
    "    else:\n",
    "        items_body.append(\"No Content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilitent Distributed Datasets\n",
    "\n",
    "<p>Conversion to RDD format by zipping a numbered index list with the list of titles above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[129] at parallelize at PythonRDD.scala:423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, u'bahia cocoa review'),\n",
       " (2, u'standard oil  to form financial unit'),\n",
       " (3, u'texas commerce bancshares  files plan'),\n",
       " (4, u'talking point/bankamerica  equity offer'),\n",
       " (5, u'national average prices for farmerowned reserve'),\n",
       " (6, u'argentine 1986/87 grain/oilseed registrations'),\n",
       " (7, u'red lion inns files plans offering'),\n",
       " (8, u'usx  debt dowgraded by moodys'),\n",
       " (9, u'champion products  approves stock split'),\n",
       " (10, u'computer terminal systems  completes sale')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = (x for x in range(1, len(items_title)+1))\n",
    "rdd = sc.parallelize(zip(mylist, items_title))\n",
    "print(rdd)\n",
    "\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index\n",
    "\n",
    "<p>Created the inverted idex using flatMap() function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'bahia', 1),\n",
       " (u'cocoa', 1),\n",
       " (u'review', 1),\n",
       " (u'standard', 2),\n",
       " (u'oil', 2),\n",
       " (u'to', 2),\n",
       " (u'form', 2),\n",
       " (u'financial', 2),\n",
       " (u'unit', 2),\n",
       " (u'texas', 3)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [(word, row[0]) for word in row[1].split()] )#\\\n",
    "           # .groupByKey() \\\n",
    "           # .map(lambda x : (x[0], list(x[1]))) \n",
    "index.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "<p><b>Abstract: </b>Using HashingTF() and IDF() functions, the index is transformed into TF-IDF.</p>\n",
    "\n",
    "<p><b>Challenges: </b> Unresolved issues are implementing querying on TFIDF RDD.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {1: 7.3886, 911486: 8.0818}),\n",
       " SparseVector(1048576, {1: 7.3886, 278288: 7.6763}),\n",
       " SparseVector(1048576, {1: 7.3886, 650410: 7.6763}),\n",
       " SparseVector(1048576, {2: 6.829, 991951: 6.9832}),\n",
       " SparseVector(1048576, {2: 6.829, 871875: 5.7304}),\n",
       " SparseVector(1048576, {2: 6.829, 725041: 3.4519}),\n",
       " SparseVector(1048576, {2: 6.829, 968518: 7.1655}),\n",
       " SparseVector(1048576, {2: 6.829, 295556: 6.1359}),\n",
       " SparseVector(1048576, {2: 6.829, 98590: 4.8629}),\n",
       " SparseVector(1048576, {3: 6.9832, 838206: 7.1655}),\n",
       " SparseVector(1048576, {3: 6.9832, 1017185: 8.0818}),\n",
       " SparseVector(1048576, {3: 6.9832, 428756: 7.6763}),\n",
       " SparseVector(1048576, {3: 6.9832, 751424: 6.6955}),\n",
       " SparseVector(1048576, {3: 6.9832, 168917: 6.4723}),\n",
       " SparseVector(1048576, {4: 7.1655, 58257: 7.6763})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(index)\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "tfidf.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'collect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-40c15cdcc4f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msorted_results\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_results_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'japan temp talk'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-40c15cdcc4f0>\u001b[0m in \u001b[0;36mget_results_tfidf\u001b[1;34m(qry, idx_body, n)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_results_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mqry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx_body\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_body\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'collect'"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_results_tfidf(qry, idx_body, n):\n",
    "    score = Counter()\n",
    "    for term in qry.collect():\n",
    "        if term in idx_body:\n",
    "            i = math.log(float(n)/(1+len(idx_body[term])))\n",
    "            for doc in idx_body[term]:\n",
    "                score[doc] += idx_body[term][doc] * i\n",
    "                \n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            results.append([x[1],x[0]])\n",
    "    \n",
    "    sorted_results = sorted(results, key=lambda t: t[0] * -1 )\n",
    "    #type(score2)\n",
    "    #print(score2)\n",
    "    return sorted_results\n",
    "\n",
    "results = get_results_tfidf('japan temp talk', tfidf, 10)\n",
    "\n",
    "print(results)\n",
    "#print_results(results,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
